{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12408077,"sourceType":"datasetVersion","datasetId":7825130}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-08T09:26:23.443795Z","iopub.execute_input":"2025-07-08T09:26:23.444392Z","iopub.status.idle":"2025-07-08T09:26:23.452709Z","shell.execute_reply.started":"2025-07-08T09:26:23.444368Z","shell.execute_reply":"2025-07-08T09:26:23.451934Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/veremi-29f-6classes/y.csv\n/kaggle/input/veremi-29f-6classes/X.csv\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\nimport time\nimport os\nimport sys\nimport joblib\nfrom datetime import datetime\n\ndef print_flush(*args, **kwargs):\n    print(*args, **kwargs)\n    sys.stdout.flush()\n\n# ========== CONFIGURATION ==========\nprint_flush(\"Starting Sender-Aware XGBoost model training...\")\n\n# ========== GPU-ENABLED XGBOOST PARAMETERS ==========\nBEST_XGB_PARAMS = {\n    'objective': 'multi:softmax',\n    'num_class': 6,\n    'eval_metric': 'mlogloss',\n    'max_depth': 8,\n    'learning_rate': 0.068,\n    'n_estimators': 501,\n    'subsample': 0.92,\n    'colsample_bytree': 0.80,\n    'random_state': 42,\n    'verbosity': 1,\n    'n_jobs': -1,\n    'tree_method': 'gpu_hist'\n}\n\n# ========== LOAD DATA ==========\nprint_flush(\"Loading data from Kaggle input...\")\nX_full = pd.read_csv('/kaggle/input/veremi-29f-6classes/X.csv')\ny_full = pd.read_csv('/kaggle/input/veremi-29f-6classes/y.csv')\n\nprint_flush(f\"Original X shape: {X_full.shape}\")\nprint_flush(f\"Original y shape: {y_full.shape}\")\n\n# Keep only the 15 required features\nREQUIRED_FEATURES = ['sendtime', 'senderpseudo', 'posx', 'posy', 'posx_n', \n                    'spdx', 'spdy', 'spdx_n', 'spdy_n', 'aclx', 'acly', \n                    'hedx', 'hedy', 'hedx_n', 'hedy_n']\n\nprint_flush(\"Filtering to 15 required features...\")\nmissing_features = [f for f in REQUIRED_FEATURES if f not in X_full.columns]\nif missing_features:\n    raise ValueError(f\"Missing required features: {missing_features}\")\n\nX_filtered = X_full[REQUIRED_FEATURES].copy()\nprint_flush(f\"Filtered X shape: {X_filtered.shape}\")\n\n# Extract y values (assuming single column)\ny_values = y_full.iloc[:, 0].values\n\n# ========== SENDER-AWARE SPLITTING ==========\ndef create_sender_aware_splits(X, y, train_val_ratio=0.80, random_state=42):\n    print_flush(\"\\n========== Sender-Aware Data Splitting ==========\")\n    senders = X['senderpseudo'].values\n    unique_senders = np.unique(senders)\n    print_flush(f\"Total unique senders: {len(unique_senders)}\")\n    \n    np.random.seed(random_state)\n    n_train_val_senders = int(train_val_ratio * len(unique_senders))\n    shuffled_senders = np.random.permutation(unique_senders)\n    train_val_senders = shuffled_senders[:n_train_val_senders]\n    test_senders = shuffled_senders[n_train_val_senders:]\n    \n    print_flush(f\"Train/Val senders: {len(train_val_senders)} ({len(train_val_senders)/len(unique_senders)*100:.1f}%)\")\n    print_flush(f\"Test senders: {len(test_senders)} ({len(test_senders)/len(unique_senders)*100:.1f}%)\")\n    \n    train_val_mask = np.isin(senders, train_val_senders)\n    test_mask = np.isin(senders, test_senders)\n    \n    X_train_val = X[train_val_mask].reset_index(drop=True)\n    y_train_val = y[train_val_mask]\n    X_test_new = X[test_mask].reset_index(drop=True)\n    y_test_new = y[test_mask]\n    \n    # Verify no sender overlap\n    train_val_sender_set = set(X_train_val['senderpseudo'].unique())\n    test_sender_set = set(X_test_new['senderpseudo'].unique())\n    overlap = len(train_val_sender_set & test_sender_set)\n    print_flush(f\"Sender overlap: {overlap} (should be 0)\")\n    \n    return X_train_val, y_train_val, X_test_new, y_test_new\n\n# Perform sender-aware split\nX_train_val, y_train_val, X_test_unseen, y_test_unseen = create_sender_aware_splits(\n    X_filtered, y_values, train_val_ratio=0.80\n)\n\n# ========== STANDARDIZATION ==========\nprint_flush(\"\\nApplying standardization to numerical features...\")\nnumerical_features = ['posx', 'posy', 'posx_n', 'spdx', 'spdy', 'spdx_n', 'spdy_n', \n                     'aclx', 'acly', 'hedx', 'hedy', 'hedx_n', 'hedy_n']\n\nscaler_initial = StandardScaler()\nX_train_val_scaled = X_train_val.copy()\nX_test_scaled = X_test_unseen.copy()\n\nX_train_val_scaled[numerical_features] = scaler_initial.fit_transform(X_train_val[numerical_features])\nX_test_scaled[numerical_features] = scaler_initial.transform(X_test_unseen[numerical_features])\n\n# ========== FEATURE SELECTION ==========\nFEATURES = ['senderpseudo', 'posx', 'posy', 'posx_n', 'spdx', 'spdy', 'hedy', 'hedx_n']\n\ndef check_and_select_features(df: pd.DataFrame, features: list, df_name: str):\n    missing = [f for f in features if f not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing features in {df_name}: {missing}\")\n    return df[features].copy()\n\nprint_flush(f\"\\nSelecting 8 core features: {FEATURES}\")\nX_train_val_selected = check_and_select_features(X_train_val_scaled, FEATURES, 'X_train_val')\nX_test_selected = check_and_select_features(X_test_scaled, FEATURES, 'X_test')\n\n# ========== FEATURE ENGINEERING ==========\ndef engineer_advanced_features(X: pd.DataFrame) -> pd.DataFrame:\n    X = X.copy()\n    print_flush(\"Starting advanced feature engineering...\")\n    \n    # Sequence and timing features\n    X['sender_sequence'] = X.groupby('senderpseudo').cumcount()\n    X['inter_arrival_time'] = X.groupby('senderpseudo')['sender_sequence'].diff().fillna(1.0)\n    \n    # Speed features\n    X['speed_magnitude'] = np.sqrt(X['spdx']**2 + X['spdy']**2)\n    X['acceleration_x'] = X.groupby('senderpseudo')['spdx'].diff().fillna(0)\n    X['acceleration_y'] = X.groupby('senderpseudo')['spdy'].diff().fillna(0)\n    X['acceleration_magnitude'] = np.sqrt(X['acceleration_x']**2 + X['acceleration_y']**2)\n    \n    # Position features\n    X['position_change_x'] = X.groupby('senderpseudo')['posx'].diff().fillna(0)\n    X['position_change_y'] = X.groupby('senderpseudo')['posy'].diff().fillna(0)\n    X['position_change_magnitude'] = np.sqrt(X['position_change_x']**2 + X['position_change_y']**2)\n    \n    # Heading features\n    X['heading_change'] = X.groupby('senderpseudo')['hedy'].diff().fillna(0)\n    X['heading_magnitude'] = np.sqrt(X['hedx_n']**2 + X['hedy']**2)\n    \n    # Consistency features\n    X['speed_consistency'] = X.groupby('senderpseudo')['speed_magnitude'].transform('std').fillna(0)\n    X['position_consistency'] = X.groupby('senderpseudo')['position_change_magnitude'].transform('std').fillna(0)\n    \n    # Time-based features\n    X['hour'] = (X['sender_sequence'] % 24)\n    X['night_hours'] = X['hour'].between(22, 5, inclusive=\"left\").astype(int)\n    \n    # Interaction features\n    X['speed_position_interaction'] = X['speed_magnitude'] * X['position_change_magnitude']\n    X['inter_arrival_speed_ratio'] = X['inter_arrival_time'] / (X['speed_magnitude'] + 1e-6)\n    \n    # Fill any remaining NaN values\n    X = X.fillna(0)\n    print_flush(f\"Feature engineering completed. New shape: {X.shape}\")\n    return X\n\nprint_flush(\"\\n========== Enhanced Feature Engineering ==========\")\nX_train_val_engineered = engineer_advanced_features(X_train_val_selected)\nX_test_engineered = engineer_advanced_features(X_test_selected)\n\nFEATURES_ENGINEERED = list(X_train_val_engineered.columns)\nprint_flush(f\"Total features after engineering: {len(FEATURES_ENGINEERED)}\")\n\n# ========== MODEL TRAINING ==========\ndef run_sender_aware_xgboost():\n    print_flush(\"\\n========== Running Sender-Aware XGBoost Model (GPU) ==========\")\n    print_flush(f\"Using {len(FEATURES_ENGINEERED)} engineered features\")\n    print_flush(\"ðŸŽ¯ Sender-aware validation: NO SENDER LEAKAGE\")\n    \n    # Class distribution analysis\n    print_flush(\"Class distribution in train/val set:\")\n    unique, counts = np.unique(y_train_val, return_counts=True)\n    for cls, count in zip(unique, counts):\n        print_flush(f\"Class {cls}: {count} samples ({count/len(y_train_val)*100:.2f}%)\")\n\n    # Calculate class weights\n    class_counts = np.bincount(y_train_val)\n    total_samples = len(y_train_val)\n    class_weights = total_samples / (len(class_counts) * class_counts)\n    print_flush(\"Class weights:\", {i: weight for i, weight in enumerate(class_weights)})\n\n    # Set up cross-validation with k=5\n    k = 5\n    group_kfold = GroupKFold(n_splits=k)\n    groups = X_train_val_engineered['senderpseudo'].values\n\n    fold_accuracies = []\n    fold_f1_scores = []\n\n    print_flush(f\"\\nStarting {k}-Fold Sender-Aware Cross Validation (GPU)...\")\n\n    params = BEST_XGB_PARAMS.copy()\n\n    # Final standardization for engineered features\n    scaler = StandardScaler()\n    X_train_val_final = scaler.fit_transform(X_train_val_engineered)\n    X_test_final = scaler.transform(X_test_engineered)\n\n    for fold, (train_index, val_index) in enumerate(group_kfold.split(X_train_val_final, y_train_val, groups=groups)):\n        print_flush(f\"\\n--- Fold {fold+1}/{k} (Sender-Aware) ---\")\n        \n        X_train_fold = X_train_val_final[train_index]\n        X_val_fold = X_train_val_final[val_index]\n        y_train_fold = y_train_val[train_index]\n        y_val_fold = y_train_val[val_index]\n        \n        # Verify sender separation\n        train_senders = set(X_train_val_engineered.iloc[train_index]['senderpseudo'].unique())\n        val_senders = set(X_train_val_engineered.iloc[val_index]['senderpseudo'].unique())\n        overlap = len(train_senders & val_senders)\n        \n        print_flush(f\"Train fold size: {X_train_fold.shape[0]} ({len(train_senders)} senders)\")\n        print_flush(f\"Val fold size: {X_val_fold.shape[0]} ({len(val_senders)} senders)\")\n        print_flush(f\"Sender overlap: {overlap} (should be 0)\")\n        \n        # Train model\n        model_fold = XGBClassifier(**params)\n        sample_weights = np.array([class_weights[y] for y in y_train_fold])\n        \n        start_time = time.time()\n        model_fold.fit(X_train_fold, y_train_fold, sample_weight=sample_weights)\n        training_time = time.time() - start_time\n        \n        print_flush(f\"Fold {fold+1} training time: {training_time:.2f} seconds\")\n        \n        # Evaluate\n        y_pred_fold = model_fold.predict(X_val_fold)\n        fold_accuracy = accuracy_score(y_val_fold, y_pred_fold)\n        fold_f1 = f1_score(y_val_fold, y_pred_fold, average='macro')\n        \n        fold_accuracies.append(fold_accuracy)\n        fold_f1_scores.append(fold_f1)\n        \n        print_flush(f\"Fold {fold+1} Validation Accuracy: {fold_accuracy:.4f}\")\n        print_flush(f\"Fold {fold+1} Validation F1-Score: {fold_f1:.4f}\")\n        \n        print_flush(f\"\\nFold {fold+1} Classification Report:\")\n        print_flush(classification_report(y_val_fold, y_pred_fold))\n\n    # Test final model performance\n    print_flush(\"\\n===== Training Final Model on All Training Data =====\")\n    final_model = XGBClassifier(**params)\n    final_model.fit(X_train_val_final, y_train_val,\n                   sample_weight=np.array([class_weights[y] for y in y_train_val]))\n    \n    # Test on unseen senders\n    y_test_pred = final_model.predict(X_test_final)\n    test_accuracy = accuracy_score(y_test_unseen, y_test_pred)\n    test_f1_macro = f1_score(y_test_unseen, y_test_pred, average='macro')\n    test_f1_weighted = f1_score(y_test_unseen, y_test_pred, average='weighted')\n    \n    print_flush(f\"\\n===== Sender-Aware XGBoost Performance =====\")\n    print_flush(f\"Validation (CV) Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies)*2:.4f})\")\n    print_flush(f\"Validation (CV) F1-Macro: {np.mean(fold_f1_scores):.4f} (+/- {np.std(fold_f1_scores)*2:.4f})\")\n    print_flush(f\"Test (Unseen Senders) Accuracy: {test_accuracy:.4f}\")\n    print_flush(f\"Test (Unseen Senders) F1-Macro: {test_f1_macro:.4f}\")\n    print_flush(f\"Test (Unseen Senders) F1-Weighted: {test_f1_weighted:.4f}\")\n\n    # Overfitting analysis\n    cv_test_gap = np.mean(fold_accuracies) - test_accuracy\n    print_flush(f\"\\n========== Overfitting Analysis ==========\")\n    print_flush(f\"CV-Test Accuracy Gap: {cv_test_gap:.4f} ({cv_test_gap*100:.2f}%)\")\n    \n    if cv_test_gap < 0.02:\n        print_flush(\"âœ… Excellent generalization - minimal overfitting\")\n    elif cv_test_gap < 0.05:\n        print_flush(\"âœ… Good generalization - acceptable overfitting\")\n    elif cv_test_gap < 0.10:\n        print_flush(\"âš ï¸ Moderate overfitting - consider regularization\")\n    else:\n        print_flush(\"âŒ Severe overfitting - model memorizing sender patterns\")\n    \n    print_flush(\"\\nTest Set (Unseen Senders) Classification Report:\")\n    print_flush(classification_report(y_test_unseen, y_test_pred))\n\n    # ========== SAVE ONLY THE TWO REQUIRED FILES ==========\n    print_flush(\"\\n===== Saving Final Model and Scaler for Website =====\")\n    joblib.dump(final_model, 'final_xgboost_model.pkl')\n    joblib.dump(scaler, 'final_xgboost_scaler.pkl')\n    \n    print_flush(\"âœ… Successfully saved:\")\n    print_flush(\"   - final_xgboost_model.pkl\")\n    print_flush(\"   - final_xgboost_scaler.pkl\")\n    print_flush(\"\\nðŸš€ These files are ready for your website deployment!\")\n\n    return final_model, scaler\n\nif __name__ == \"__main__\":\n    print_flush(\"Starting Sender-Aware XGBoost 6-Class model training...\")\n    print_flush(\"ðŸŽ¯ TRUE SENDER-AWARE VALIDATION - NO SENDER LEAKAGE\")\n    print_flush(\"âœ… 80% senders for train/val, 20% senders for test\")\n    print_flush(\"âœ… GroupKFold with k=5 ensures no sender appears in both train and val\")\n    \n    final_model, scaler = run_sender_aware_xgboost()\n    \n    print_flush(\"\\nðŸŽ‰ Training completed successfully!\")\n    print_flush(\"ðŸ“¦ Only the required files have been saved for website deployment\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T09:28:28.410006Z","iopub.execute_input":"2025-07-08T09:28:28.410285Z","iopub.status.idle":"2025-07-08T09:35:44.295496Z","shell.execute_reply.started":"2025-07-08T09:28:28.410263Z","shell.execute_reply":"2025-07-08T09:35:44.294906Z"}},"outputs":[{"name":"stdout","text":"Starting Sender-Aware XGBoost model training...\nLoading data from Kaggle input...\nOriginal X shape: (3194808, 29)\nOriginal y shape: (3194808, 1)\nFiltering to 15 required features...\nFiltered X shape: (3194808, 15)\n\n========== Sender-Aware Data Splitting ==========\nTotal unique senders: 118909\nTrain/Val senders: 95127 (80.0%)\nTest senders: 23782 (20.0%)\nSender overlap: 0 (should be 0)\n\nApplying standardization to numerical features...\n\nSelecting 8 core features: ['senderpseudo', 'posx', 'posy', 'posx_n', 'spdx', 'spdy', 'hedy', 'hedx_n']\n\n========== Enhanced Feature Engineering ==========\nStarting advanced feature engineering...\nFeature engineering completed. New shape: (2560342, 25)\nStarting advanced feature engineering...\nFeature engineering completed. New shape: (634466, 25)\nTotal features after engineering: 25\nStarting Sender-Aware XGBoost 6-Class model training...\nðŸŽ¯ TRUE SENDER-AWARE VALIDATION - NO SENDER LEAKAGE\nâœ… 80% senders for train/val, 20% senders for test\nâœ… GroupKFold with k=5 ensures no sender appears in both train and val\n\n========== Running Sender-Aware XGBoost Model (GPU) ==========\nUsing 25 engineered features\nðŸŽ¯ Sender-aware validation: NO SENDER LEAKAGE\nClass distribution in train/val set:\nClass 0: 1516446 samples (59.23%)\nClass 1: 141287 samples (5.52%)\nClass 2: 210772 samples (8.23%)\nClass 3: 102389 samples (4.00%)\nClass 4: 103630 samples (4.05%)\nClass 5: 485818 samples (18.97%)\nClass weights: {0: 0.2813972054835231, 1: 3.0202613592663634, 2: 2.024574737947482, 3: 4.167671006325549, 4: 4.117761909357007, 5: 0.8783611695463459}\n\nStarting 5-Fold Sender-Aware Cross Validation (GPU)...\n\n--- Fold 1/5 (Sender-Aware) ---\nTrain fold size: 2048273 (76157 senders)\nVal fold size: 512069 (18970 senders)\nSender overlap: 0 (should be 0)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:29:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 1 training time: 63.04 seconds\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:30:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:30:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 1 Validation Accuracy: 0.9474\nFold 1 Validation F1-Score: 0.8910\n\nFold 1 Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98    300198\n           1       0.97      0.86      0.91     32312\n           2       0.87      0.90      0.89     44596\n           3       0.86      0.90      0.88     20478\n           4       0.85      0.70      0.77      9336\n           5       0.95      0.89      0.92    105149\n\n    accuracy                           0.95    512069\n   macro avg       0.91      0.87      0.89    512069\nweighted avg       0.95      0.95      0.95    512069\n\n\n--- Fold 2/5 (Sender-Aware) ---\nTrain fold size: 2048273 (76087 senders)\nVal fold size: 512069 (19040 senders)\nSender overlap: 0 (should be 0)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:30:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 2 training time: 62.82 seconds\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:31:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 2 Validation Accuracy: 0.9391\nFold 2 Validation F1-Score: 0.8968\n\nFold 2 Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      0.99      0.98    304049\n           1       0.95      0.83      0.88     28641\n           2       0.78      0.91      0.84     38985\n           3       0.85      0.91      0.88     20200\n           4       0.95      0.88      0.91     25058\n           5       0.95      0.83      0.89     95136\n\n    accuracy                           0.94    512069\n   macro avg       0.91      0.89      0.90    512069\nweighted avg       0.94      0.94      0.94    512069\n\n\n--- Fold 3/5 (Sender-Aware) ---\nTrain fold size: 2048274 (76088 senders)\nVal fold size: 512068 (19039 senders)\nSender overlap: 0 (should be 0)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:31:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 3 training time: 62.69 seconds\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:32:12] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 3 Validation Accuracy: 0.9356\nFold 3 Validation F1-Score: 0.8869\n\nFold 3 Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98    305147\n           1       0.95      0.80      0.87     25105\n           2       0.78      0.90      0.84     44012\n           3       0.85      0.90      0.87     19235\n           4       0.90      0.87      0.88     14584\n           5       0.94      0.82      0.88    103985\n\n    accuracy                           0.94    512068\n   macro avg       0.90      0.88      0.89    512068\nweighted avg       0.94      0.94      0.93    512068\n\n\n--- Fold 4/5 (Sender-Aware) ---\nTrain fold size: 2048274 (76088 senders)\nVal fold size: 512068 (19039 senders)\nSender overlap: 0 (should be 0)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:32:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 4 training time: 62.84 seconds\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:33:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 4 Validation Accuracy: 0.9381\nFold 4 Validation F1-Score: 0.8956\n\nFold 4 Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98    299812\n           1       0.96      0.85      0.90     27427\n           2       0.83      0.88      0.85     46480\n           3       0.85      0.89      0.87     20648\n           4       0.96      0.82      0.88     26449\n           5       0.93      0.85      0.89     91252\n\n    accuracy                           0.94    512068\n   macro avg       0.92      0.88      0.90    512068\nweighted avg       0.94      0.94      0.94    512068\n\n\n--- Fold 5/5 (Sender-Aware) ---\nTrain fold size: 2048274 (76088 senders)\nVal fold size: 512068 (19039 senders)\nSender overlap: 0 (should be 0)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:33:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 5 training time: 62.88 seconds\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:34:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 5 Validation Accuracy: 0.9454\nFold 5 Validation F1-Score: 0.9061\n\nFold 5 Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98    307240\n           1       0.96      0.84      0.90     27802\n           2       0.83      0.91      0.87     36699\n           3       0.88      0.90      0.89     21828\n           4       0.97      0.84      0.90     28203\n           5       0.95      0.86      0.90     90296\n\n    accuracy                           0.95    512068\n   macro avg       0.93      0.89      0.91    512068\nweighted avg       0.95      0.95      0.94    512068\n\n\n===== Training Final Model on All Training Data =====\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:34:29] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [09:35:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"\n===== Sender-Aware XGBoost Performance =====\nValidation (CV) Accuracy: 0.9411 (+/- 0.0090)\nValidation (CV) F1-Macro: 0.8953 (+/- 0.0129)\nTest (Unseen Senders) Accuracy: 0.9417\nTest (Unseen Senders) F1-Macro: 0.8987\nTest (Unseen Senders) F1-Weighted: 0.9410\n\n========== Overfitting Analysis ==========\nCV-Test Accuracy Gap: -0.0006 (-0.06%)\nâœ… Excellent generalization - minimal overfitting\n\nTest Set (Unseen Senders) Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.97      1.00      0.98    384093\n           1       0.95      0.83      0.89     32365\n           2       0.80      0.92      0.85     48489\n           3       0.86      0.91      0.89     27294\n           4       0.92      0.88      0.90     27675\n           5       0.96      0.82      0.89    114550\n\n    accuracy                           0.94    634466\n   macro avg       0.91      0.89      0.90    634466\nweighted avg       0.94      0.94      0.94    634466\n\n\n===== Saving Final Model and Scaler for Website =====\nâœ… Successfully saved:\n   - final_xgboost_model.pkl\n   - final_xgboost_scaler.pkl\n\nðŸš€ These files are ready for your website deployment!\n\nðŸŽ‰ Training completed successfully!\nðŸ“¦ Only the required files have been saved for website deployment\n","output_type":"stream"}],"execution_count":7}]}